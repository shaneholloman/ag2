# Copyright (c) 2023 - 2026, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

"""Tests for autogen.opentelemetry.instrumentators.llm_wrapper module (instrument_llm_wrapper)."""

import json
from dataclasses import dataclass
from unittest.mock import MagicMock, patch

import pytest
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import SimpleSpanProcessor

from autogen.oai.client import OpenAIWrapper
from autogen.opentelemetry import instrument_llm_wrapper
from test.opentelemetry.conftest import InMemorySpanExporter


@pytest.fixture()
def otel_setup():
    """Create an in-memory OTEL exporter/provider for capturing spans."""
    exporter = InMemorySpanExporter()
    provider = TracerProvider()
    provider.add_span_processor(SimpleSpanProcessor(exporter))
    return exporter, provider


@pytest.fixture(autouse=True)
def _restore_openai_wrapper_create():
    """Restore OpenAIWrapper.create after each test to undo monkey-patching."""
    original = OpenAIWrapper.create
    yield
    OpenAIWrapper.create = original


# ---------------------------------------------------------------------------
# Helper: fake LLM response
# ---------------------------------------------------------------------------
@dataclass
class FakeUsage:
    prompt_tokens: int = 10
    completion_tokens: int = 20


@dataclass
class FakeMessage:
    content: str = "test response"
    tool_calls: list = None

    def __post_init__(self):
        if self.tool_calls is None:
            self.tool_calls = []


@dataclass
class FakeChoice:
    message: FakeMessage = None
    finish_reason: str = "stop"

    def __post_init__(self):
        if self.message is None:
            self.message = FakeMessage()


@dataclass
class FakeResponse:
    model: str = "gpt-4"
    usage: FakeUsage = None
    choices: list = None
    cost: float = 0.01

    def __post_init__(self):
        if self.usage is None:
            self.usage = FakeUsage()
        if self.choices is None:
            self.choices = [FakeChoice()]


# ---------------------------------------------------------------------------
# Basic wrapping
# ---------------------------------------------------------------------------
class TestInstrumentLlmWrapperBasic:
    """Tests that instrument_llm_wrapper wraps OpenAIWrapper.create."""

    def test_wraps_create_method(self, otel_setup) -> None:
        _, provider = otel_setup
        instrument_llm_wrapper(tracer_provider=provider)
        assert hasattr(OpenAIWrapper.create, "__otel_wrapped__")

    def test_idempotency(self, otel_setup) -> None:
        _, provider = otel_setup
        instrument_llm_wrapper(tracer_provider=provider)
        first_create = OpenAIWrapper.create
        instrument_llm_wrapper(tracer_provider=provider)
        # Should not re-wrap; same function object
        assert OpenAIWrapper.create is first_create


# ---------------------------------------------------------------------------
# Span creation
# ---------------------------------------------------------------------------
class TestLlmSpanCreation:
    """Tests that wrapped create() produces correct LLM spans."""

    def test_creates_llm_span(self, otel_setup) -> None:
        exporter, provider = otel_setup
        instrument_llm_wrapper(tracer_provider=provider)

        with patch.object(OpenAIWrapper, "create", wraps=OpenAIWrapper.create):
            # We need to call the actual wrapped create, which calls original_create
            # Mock the original create to return a fake response
            wrapper = MagicMock(spec=OpenAIWrapper)
            wrapper._config_list = [{"model": "gpt-4"}]

            # Directly call the traced_create function (which is now OpenAIWrapper.create)
            with patch.object(OpenAIWrapper, "__init__", return_value=None):
                # We need to work at a lower level - just test the span is created
                # by calling the patched method with a mock self
                pass

        # Simpler approach: directly test the patched method
        exporter.clear()
        instrument_llm_wrapper(tracer_provider=provider)

        # Capture the traced_create function
        traced_create = OpenAIWrapper.create

        # Create a mock wrapper instance
        mock_wrapper = MagicMock()
        mock_wrapper._config_list = [{"model": "gpt-4"}]

        # The traced_create calls original_create(self, **config)
        # We need to mock original_create - it's captured in the closure
        # Instead, let's patch at module level
        # The simplest way is to verify the function has the __otel_wrapped__ flag
        assert hasattr(traced_create, "__otel_wrapped__")

    def test_span_attributes_with_model(self, otel_setup) -> None:
        """Test that traced_create produces a span with the expected attributes."""
        exporter, provider = otel_setup

        # Save original create before instrumentation
        original_create = OpenAIWrapper.create

        instrument_llm_wrapper(tracer_provider=provider)

        fake_resp = FakeResponse(model="gpt-4")

        # Replace the original_create captured in the closure by re-instrumenting
        # with a controlled original
        exporter.clear()
        OpenAIWrapper.create = original_create
        instrument_llm_wrapper(tracer_provider=provider)

        # Get the traced_create and call it with a mock self,
        # but we need to control what original_create does.
        # Since original_create is captured in the closure, we can't patch it directly.
        # Instead, test that _set_llm_response_attributes correctly sets model attributes.
        from autogen.opentelemetry.instrumentators.llm_wrapper import _set_llm_response_attributes

        span = MagicMock()
        _set_llm_response_attributes(span, fake_resp)
        span.set_attribute.assert_any_call("gen_ai.response.model", "gpt-4")


class TestLlmWrapperSpanWithMocking:
    """Tests using a direct approach to verify span attributes are set."""

    def _call_traced_create(self, otel_setup, config_list, create_kwargs, fake_response):
        """Helper to call the traced create and return spans."""
        exporter, provider = otel_setup
        exporter.clear()

        # Save and instrument
        original_create = OpenAIWrapper.create
        if hasattr(original_create, "__otel_wrapped__"):
            # Already wrapped from a previous test - unwrap
            OpenAIWrapper.create = original_create
            # Fallback: we rely on the autouse fixture to restore

        # Create a clean instrument
        instrument_llm_wrapper(tracer_provider=provider)

        # We need to replace the inner original_create call.
        # The traced_create function captured original_create in its closure.
        # Let's monkeypatch at the module level where it's used.
        # We'll create a wrapper that replaces the inner call
        mock_self = MagicMock()
        mock_self._config_list = config_list

        # Since we can't easily replace the closure variable, we'll just verify
        # the function is marked as wrapped and test the attribute-setting logic
        # via the _set_llm_response_attributes function directly
        return exporter.get_finished_spans()

    def test_set_llm_response_attributes_model(self) -> None:
        """Test _set_llm_response_attributes sets response model."""
        from autogen.opentelemetry.instrumentators.llm_wrapper import _set_llm_response_attributes

        span = MagicMock()
        resp = FakeResponse(model="gpt-4-turbo")
        _set_llm_response_attributes(span, resp)
        span.set_attribute.assert_any_call("gen_ai.response.model", "gpt-4-turbo")

    def test_set_llm_response_attributes_token_usage(self) -> None:
        """Test _set_llm_response_attributes sets token counts."""
        from autogen.opentelemetry.instrumentators.llm_wrapper import _set_llm_response_attributes

        span = MagicMock()
        resp = FakeResponse()
        resp.usage = FakeUsage(prompt_tokens=100, completion_tokens=50)
        _set_llm_response_attributes(span, resp)
        span.set_attribute.assert_any_call("gen_ai.usage.input_tokens", 100)
        span.set_attribute.assert_any_call("gen_ai.usage.output_tokens", 50)

    def test_set_llm_response_attributes_finish_reasons(self) -> None:
        """Test _set_llm_response_attributes sets finish reasons."""
        from autogen.opentelemetry.instrumentators.llm_wrapper import _set_llm_response_attributes

        span = MagicMock()
        resp = FakeResponse()
        resp.choices = [FakeChoice(finish_reason="stop"), FakeChoice(finish_reason="length")]
        _set_llm_response_attributes(span, resp)
        # Should be JSON array of reasons
        calls = {call.args[0]: call.args[1] for call in span.set_attribute.call_args_list}
        assert "gen_ai.response.finish_reasons" in calls
        reasons = json.loads(calls["gen_ai.response.finish_reasons"])
        assert "stop" in reasons
        assert "length" in reasons

    def test_set_llm_response_attributes_cost(self) -> None:
        """Test _set_llm_response_attributes sets cost."""
        from autogen.opentelemetry.instrumentators.llm_wrapper import _set_llm_response_attributes

        span = MagicMock()
        resp = FakeResponse(cost=0.05)
        _set_llm_response_attributes(span, resp)
        span.set_attribute.assert_any_call("gen_ai.usage.cost", 0.05)

    def test_set_llm_response_attributes_no_usage(self) -> None:
        """Test _set_llm_response_attributes with no usage data."""
        from autogen.opentelemetry.instrumentators.llm_wrapper import _set_llm_response_attributes

        span = MagicMock()
        resp = FakeResponse()
        resp.usage = None
        _set_llm_response_attributes(span, resp)
        # Should not crash; model and cost should still be set
        calls = {call.args[0] for call in span.set_attribute.call_args_list}
        assert "gen_ai.response.model" in calls
        assert "gen_ai.usage.input_tokens" not in calls

    def test_set_llm_response_attributes_no_choices(self) -> None:
        """Test _set_llm_response_attributes with no choices."""
        from autogen.opentelemetry.instrumentators.llm_wrapper import _set_llm_response_attributes

        span = MagicMock()
        resp = FakeResponse()
        resp.choices = []
        _set_llm_response_attributes(span, resp)
        calls = {call.args[0] for call in span.set_attribute.call_args_list}
        assert "gen_ai.response.finish_reasons" not in calls

    def test_set_llm_response_attributes_capture_messages(self) -> None:
        """Test _set_llm_response_attributes with capture_messages=True."""
        from autogen.opentelemetry.instrumentators.llm_wrapper import _set_llm_response_attributes

        span = MagicMock()
        resp = FakeResponse()
        resp.choices = [FakeChoice(message=FakeMessage(content="Hello!"))]
        _set_llm_response_attributes(span, resp, capture_messages=True)
        calls = {call.args[0]: call.args[1] for call in span.set_attribute.call_args_list}
        assert "gen_ai.output.messages" in calls
        output_msgs = json.loads(calls["gen_ai.output.messages"])
        assert len(output_msgs) >= 1
        assert output_msgs[0]["role"] == "assistant"

    def test_set_llm_response_attributes_no_capture_messages(self) -> None:
        """Test _set_llm_response_attributes with capture_messages=False (default)."""
        from autogen.opentelemetry.instrumentators.llm_wrapper import _set_llm_response_attributes

        span = MagicMock()
        resp = FakeResponse()
        _set_llm_response_attributes(span, resp, capture_messages=False)
        calls = {call.args[0] for call in span.set_attribute.call_args_list}
        assert "gen_ai.output.messages" not in calls

    def test_set_llm_response_attributes_with_tool_calls(self) -> None:
        """Test _set_llm_response_attributes captures tool calls in output messages."""
        from autogen.opentelemetry.instrumentators.llm_wrapper import _set_llm_response_attributes

        @dataclass
        class FakeFunction:
            name: str = "get_weather"
            arguments: str = '{"city": "London"}'

        @dataclass
        class FakeToolCall:
            id: str = "call_123"
            function: FakeFunction = None

            def __post_init__(self):
                if self.function is None:
                    self.function = FakeFunction()

        msg = FakeMessage(content="", tool_calls=[FakeToolCall()])
        choice = FakeChoice(message=msg, finish_reason="tool_calls")
        resp = FakeResponse(choices=[choice])

        span = MagicMock()
        _set_llm_response_attributes(span, resp, capture_messages=True)
        calls = {call.args[0]: call.args[1] for call in span.set_attribute.call_args_list}
        assert "gen_ai.output.messages" in calls
        output_msgs = json.loads(calls["gen_ai.output.messages"])
        assert len(output_msgs) >= 1
        # The output should contain tool_call parts
        parts = output_msgs[0].get("parts", [])
        tool_call_parts = [p for p in parts if p.get("type") == "tool_call"]
        assert len(tool_call_parts) == 1
        assert tool_call_parts[0]["name"] == "get_weather"

    def test_set_llm_response_attributes_no_model(self) -> None:
        """Test _set_llm_response_attributes when response has no model."""
        from autogen.opentelemetry.instrumentators.llm_wrapper import _set_llm_response_attributes

        span = MagicMock()
        resp = FakeResponse(model=None)
        _set_llm_response_attributes(span, resp)
        calls = {call.args[0] for call in span.set_attribute.call_args_list}
        assert "gen_ai.response.model" not in calls

    def test_set_llm_response_attributes_no_cost(self) -> None:
        """Test _set_llm_response_attributes when response has no cost."""
        from autogen.opentelemetry.instrumentators.llm_wrapper import _set_llm_response_attributes

        span = MagicMock()
        resp = FakeResponse(cost=None)
        _set_llm_response_attributes(span, resp)
        calls = {call.args[0] for call in span.set_attribute.call_args_list}
        assert "gen_ai.usage.cost" not in calls

    def test_set_llm_response_attributes_completion_tokens_none(self) -> None:
        """Test _set_llm_response_attributes when completion_tokens is None."""
        from autogen.opentelemetry.instrumentators.llm_wrapper import _set_llm_response_attributes

        span = MagicMock()
        usage = FakeUsage(prompt_tokens=100, completion_tokens=None)
        resp = FakeResponse()
        resp.usage = usage
        _set_llm_response_attributes(span, resp)
        calls = {call.args[0]: call.args[1] for call in span.set_attribute.call_args_list}
        # completion_tokens=None should map to output_tokens=0
        assert calls.get("gen_ai.usage.output_tokens") == 0


# ---------------------------------------------------------------------------
# traced_create end-to-end tests
# ---------------------------------------------------------------------------
class TestTracedCreateEndToEnd:
    """Tests the full traced_create flow by replacing OpenAIWrapper.create before instrumenting."""

    def test_traced_create_produces_llm_span(self, otel_setup) -> None:
        exporter, provider = otel_setup

        fake_resp = FakeResponse(model="gpt-4")

        # Replace OpenAIWrapper.create with a mock that returns fake_resp
        OpenAIWrapper.create = lambda self, **config: fake_resp

        # Now instrument (this wraps our mock)
        instrument_llm_wrapper(tracer_provider=provider)

        # Call it with a mock self
        wrapper = MagicMock()
        wrapper._config_list = [{"model": "gpt-4"}]
        OpenAIWrapper.create(wrapper, messages=[{"role": "user", "content": "hi"}])

        spans = exporter.get_finished_spans()
        llm_spans = [s for s in spans if s.attributes.get("ag2.span.type") == "llm"]
        assert len(llm_spans) == 1
        assert llm_spans[0].name == "chat gpt-4"
        assert llm_spans[0].attributes["gen_ai.provider.name"] == "openai"

    def test_traced_create_captures_messages_when_enabled(self, otel_setup) -> None:
        exporter, provider = otel_setup

        fake_resp = FakeResponse(model="gpt-4")

        OpenAIWrapper.create = lambda self, **config: fake_resp

        # Instrument with capture_messages=True
        instrument_llm_wrapper(tracer_provider=provider, capture_messages=True)

        wrapper = MagicMock()
        wrapper._config_list = [{"model": "gpt-4"}]
        OpenAIWrapper.create(wrapper, messages=[{"role": "user", "content": "hello world"}])

        spans = exporter.get_finished_spans()
        llm_spans = [s for s in spans if s.attributes.get("ag2.span.type") == "llm"]
        assert len(llm_spans) == 1
        span = llm_spans[0]
        # Should have input messages
        assert "gen_ai.input.messages" in span.attributes
        input_msgs = json.loads(span.attributes["gen_ai.input.messages"])
        assert len(input_msgs) >= 1
        # Should have output messages
        assert "gen_ai.output.messages" in span.attributes

    def test_traced_create_does_not_capture_messages_by_default(self, otel_setup) -> None:
        exporter, provider = otel_setup

        fake_resp = FakeResponse(model="gpt-4")

        OpenAIWrapper.create = lambda self, **config: fake_resp

        # Instrument with default capture_messages=False
        instrument_llm_wrapper(tracer_provider=provider)

        wrapper = MagicMock()
        wrapper._config_list = [{"model": "gpt-4"}]
        OpenAIWrapper.create(wrapper, messages=[{"role": "user", "content": "secret data"}])

        spans = exporter.get_finished_spans()
        llm_spans = [s for s in spans if s.attributes.get("ag2.span.type") == "llm"]
        assert len(llm_spans) == 1
        span = llm_spans[0]
        assert "gen_ai.input.messages" not in span.attributes
        assert "gen_ai.output.messages" not in span.attributes

    def test_traced_create_records_error_on_exception(self, otel_setup) -> None:
        exporter, provider = otel_setup

        def failing_create(self, **config):
            raise ConnectionError("API unreachable")

        OpenAIWrapper.create = failing_create

        instrument_llm_wrapper(tracer_provider=provider)

        wrapper = MagicMock()
        wrapper._config_list = [{"model": "gpt-4"}]

        with pytest.raises(ConnectionError, match="API unreachable"):
            OpenAIWrapper.create(wrapper, messages=[{"role": "user", "content": "hi"}])

        spans = exporter.get_finished_spans()
        llm_spans = [s for s in spans if s.attributes.get("ag2.span.type") == "llm"]
        assert len(llm_spans) == 1
        span = llm_spans[0]
        assert span.attributes.get("error.type") == "ConnectionError"

    def test_traced_create_sets_request_params(self, otel_setup) -> None:
        exporter, provider = otel_setup

        fake_resp = FakeResponse(model="gpt-4")

        OpenAIWrapper.create = lambda self, **config: fake_resp

        instrument_llm_wrapper(tracer_provider=provider)

        wrapper = MagicMock()
        wrapper._config_list = [{"model": "gpt-4"}]
        OpenAIWrapper.create(
            wrapper,
            messages=[{"role": "user", "content": "hi"}],
            temperature=0.7,
            max_tokens=100,
            top_p=0.9,
        )

        spans = exporter.get_finished_spans()
        llm_spans = [s for s in spans if s.attributes.get("ag2.span.type") == "llm"]
        assert len(llm_spans) == 1
        span = llm_spans[0]
        assert span.attributes.get("gen_ai.request.temperature") == 0.7
        assert span.attributes.get("gen_ai.request.max_tokens") == 100
        assert span.attributes.get("gen_ai.request.top_p") == 0.9

    def test_traced_create_sets_response_token_usage(self, otel_setup) -> None:
        exporter, provider = otel_setup

        fake_resp = FakeResponse(model="gpt-4")
        fake_resp.usage = FakeUsage(prompt_tokens=50, completion_tokens=25)

        OpenAIWrapper.create = lambda self, **config: fake_resp

        instrument_llm_wrapper(tracer_provider=provider)

        wrapper = MagicMock()
        wrapper._config_list = [{"model": "gpt-4"}]
        OpenAIWrapper.create(wrapper, messages=[{"role": "user", "content": "hi"}])

        spans = exporter.get_finished_spans()
        llm_spans = [s for s in spans if s.attributes.get("ag2.span.type") == "llm"]
        assert len(llm_spans) == 1
        span = llm_spans[0]
        assert span.attributes.get("gen_ai.usage.input_tokens") == 50
        assert span.attributes.get("gen_ai.usage.output_tokens") == 25

    def test_traced_create_sets_model_and_operation(self, otel_setup) -> None:
        exporter, provider = otel_setup

        fake_resp = FakeResponse(model="gpt-4-turbo")

        OpenAIWrapper.create = lambda self, **config: fake_resp

        instrument_llm_wrapper(tracer_provider=provider)

        wrapper = MagicMock()
        wrapper._config_list = [{"model": "gpt-4-turbo"}]
        OpenAIWrapper.create(wrapper, messages=[{"role": "user", "content": "hi"}])

        spans = exporter.get_finished_spans()
        llm_spans = [s for s in spans if s.attributes.get("ag2.span.type") == "llm"]
        assert len(llm_spans) == 1
        span = llm_spans[0]
        assert span.attributes["gen_ai.operation.name"] == "chat"
        assert span.attributes["gen_ai.request.model"] == "gpt-4-turbo"
        assert span.attributes["gen_ai.response.model"] == "gpt-4-turbo"
        assert span.name == "chat gpt-4-turbo"

    def test_traced_create_sets_agent_name(self, otel_setup) -> None:
        exporter, provider = otel_setup

        fake_resp = FakeResponse(model="gpt-4")

        OpenAIWrapper.create = lambda self, **config: fake_resp

        instrument_llm_wrapper(tracer_provider=provider)

        wrapper = MagicMock()
        wrapper._config_list = [{"model": "gpt-4"}]

        agent_mock = MagicMock()
        agent_mock.name = "my_assistant"
        OpenAIWrapper.create(wrapper, messages=[{"role": "user", "content": "hi"}], agent=agent_mock)

        spans = exporter.get_finished_spans()
        llm_spans = [s for s in spans if s.attributes.get("ag2.span.type") == "llm"]
        assert len(llm_spans) == 1
        span = llm_spans[0]
        assert span.attributes.get("gen_ai.agent.name") == "my_assistant"
