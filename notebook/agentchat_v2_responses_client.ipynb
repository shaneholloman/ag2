{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Responses API V2 Client - Complete Guide\n",
    "\n",
    "This notebook demonstrates the `OpenAIResponsesV2Client` which implements the new OpenAI Responses API with rich `UnifiedResponse` objects.\n",
    "\n",
    "## Key Features\n",
    "\n",
    "- **Stateful Conversations**: Maintain conversation context via `previous_response_id`\n",
    "- **Built-in Tools**: Web search, image generation, apply_patch\n",
    "- **Rich Content Blocks**: TextContent, ReasoningContent, CitationContent, ImageContent, ToolCallContent\n",
    "- **Multimodal Support**: Send and receive images\n",
    "- **Structured Output**: Pydantic models and JSON schema support\n",
    "- **Cost Tracking**: Token and image generation cost tracking\n",
    "- **Agent Integration**: Works with AG2 agents for single, two-agent, and group chat\n",
    "\n",
    "## Requirements\n",
    "\n",
    "AG2 requires `Python>=3.10`. Install the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install \"ag2[openai]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Set your OpenAI API key as an environment variable or pass it directly to the client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your API key (or use environment variable OPENAI_API_KEY)\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"sk-...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 1. Basic Usage\n",
    "\n",
    "The `OpenAIResponsesV2Client` returns rich `UnifiedResponse` objects with typed content blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen.llm_clients.openai_responses_v2 import OpenAIResponsesV2Client\n",
    "\n",
    "# Create the V2 client\n",
    "client = OpenAIResponsesV2Client()\n",
    "\n",
    "# Make a simple request\n",
    "response = client.create({\n",
    "    \"model\": \"gpt-5-nano\",\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"how are you? tell me about yourself? and what is a machine? in one line\"}\n",
    "    ],\n",
    "})\n",
    "\n",
    "# Access the response\n",
    "print(f\"Response ID: {response.id}\")\n",
    "print(f\"Model: {response.model}\")\n",
    "print(f\"Content: {response.messages[0].get_text()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding UnifiedResponse Structure\n",
    "\n",
    "The `UnifiedResponse` contains rich, typed content blocks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen.llm_clients.models.content_blocks import (\n",
    "    ReasoningContent,\n",
    "    TextContent,\n",
    ")\n",
    "\n",
    "# Inspect the response structure\n",
    "print(f\"Number of messages: {len(response.messages)}\")\n",
    "print(f\"Usage: {response.usage}\")\n",
    "print(f\"Cost: ${response.cost:.6f}\")\n",
    "\n",
    "# Iterate through content blocks\n",
    "for msg in response.messages:\n",
    "    print(f\"\\nRole: {msg.role}\")\n",
    "    for block in msg.content:\n",
    "        if isinstance(block, TextContent):\n",
    "            print(f\"  Text: {block.text[:100]}...\" if len(block.text) > 100 else f\"  Text: {block.text}\")\n",
    "        elif isinstance(block, ReasoningContent):\n",
    "            # Note that OpenAI may not return reasoning content with its API\n",
    "            print(f\"  Reasoning: {block.text[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2. Stateful Conversations\n",
    "\n",
    "The Responses API is **stateful** - it maintains conversation context server-side using `previous_response_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new client for stateful conversation\n",
    "stateful_client = OpenAIResponsesV2Client()\n",
    "\n",
    "# First message\n",
    "response1 = stateful_client.create({\n",
    "    \"model\": \"gpt-4.1\",\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"My name is Alice. Remember this.\"}],\n",
    "})\n",
    "print(f\"Response 1: {response1.messages[0].get_text()}\")\n",
    "print(f\"Response ID: {response1.id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second message - the client automatically tracks state\n",
    "response2 = stateful_client.create({\"model\": \"gpt-4.1\", \"messages\": [{\"role\": \"user\", \"content\": \"What is my name?\"}]})\n",
    "print(f\"Response 2: {response2.messages[0].get_text()}\")\n",
    "print(\"\\nThe model remembered the context from the previous turn!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset conversation state to start fresh\n",
    "stateful_client.reset_conversation()\n",
    "\n",
    "response3 = stateful_client.create({\"model\": \"gpt-4.1\", \"messages\": [{\"role\": \"user\", \"content\": \"What is my name?\"}]})\n",
    "print(f\"After reset: {response3.messages[0].get_text()}\")\n",
    "print(\"\\nThe model no longer has context from previous conversation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual State Control\n",
    "\n",
    "You can also manually control the conversation state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get current state\n",
    "current_state = stateful_client._get_previous_response_id()\n",
    "print(f\"Current state: {current_state}\")\n",
    "\n",
    "# Get a fresh response ID\n",
    "response_a = client.create({\"model\": \"gpt-4.1\", \"messages\": [{\"role\": \"user\", \"content\": \"Hello, my name is Alice\"}]})\n",
    "\n",
    "response_a_id = client._get_previous_response_id()\n",
    "\n",
    "response_b = client.create({\"model\": \"gpt-4.1\", \"messages\": [{\"role\": \"user\", \"content\": \"Hello, my name is Hatter\"}]})\n",
    "\n",
    "response_b_id = client._get_previous_response_id()\n",
    "\n",
    "# Use the fresh ID immediately\n",
    "client._set_previous_response_id(response_a.id)\n",
    "response_a1 = client.create({\n",
    "    \"model\": \"gpt-4.1\",\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"What's my name?\"}],\n",
    "    # \"previous_response_id\": response_a.id  # Use the real, fresh ID\n",
    "})\n",
    "\n",
    "response_a1.messages[0].get_text()\n",
    "\n",
    "client._set_previous_response_id(response_b.id)\n",
    "response_b1 = client.create({\n",
    "    \"model\": \"gpt-4.1\",\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"What's my name?\"}],\n",
    "    # \"previous_response_id\": response_b.id  # Use the real, fresh ID\n",
    "})\n",
    "response_a1_id = client._get_previous_response_id()\n",
    "response_b1_id = client._get_previous_response_id()\n",
    "\n",
    "print(\"response_a1_id\", response_a1_id)\n",
    "print(\"response_b1_id\", response_b1_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 3. Multimodal Support\n",
    "\n",
    "Send images in your messages using various formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a multimodal message with an image URL\n",
    "multimodal_message = OpenAIResponsesV2Client.create_multimodal_message(\n",
    "    text=\"What do you see in this image?\",\n",
    "    images=[\"https://images.unsplash.com/photo-1587300003388-59208cc962cb?w=400\"],\n",
    "    role=\"user\",\n",
    ")\n",
    "\n",
    "print(\"Multimodal message structure:\")\n",
    "print(multimodal_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send multimodal request\n",
    "mm_client = OpenAIResponsesV2Client()\n",
    "\n",
    "response = mm_client.create({\n",
    "    \"model\": \"gpt-4.1\",  # Use a vision-capable model\n",
    "    \"messages\": [multimodal_message],\n",
    "})\n",
    "\n",
    "print(f\"Image description: {response.messages[0].get_text()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 4. Built-in Tools\n",
    "\n",
    "The Responses API provides built-in tools that don't require function definitions.\n",
    "supports an array of built in tools : [\"web_search\", \"image_generation\", \"apply_patch\", \"apply_patch_async\", \"shell_tool\"]\n",
    "- web_search - Enables the model to search the web for real-time information and returns results with citations.\n",
    "- image_generation - Allows the model to generate images from text descriptions using DALL-E or GPT-Image models.\n",
    "- apply_patch - Enables file operations (create, update, delete files) in a workspace directory with path restrictions.\n",
    "- apply_patch_async - Same as apply_patch but executes file operations asynchronously for better performance.\n",
    "- shell - Executes shell commands with configurable sandboxing, command filtering, and security restrictions.\n",
    "## 4.1 Web Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable web search\n",
    "search_client = OpenAIResponsesV2Client()\n",
    "\n",
    "response = search_client.create({\n",
    "    \"model\": \"gpt-4.1\",\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"What is the latest news about AI?\"}],\n",
    "    \"built_in_tools\": [\"web_search\"],\n",
    "})\n",
    "\n",
    "print(f\"Response: {response.messages[0].get_text()[:500]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract citations from the response\n",
    "citations = OpenAIResponsesV2Client.get_citations(response)\n",
    "\n",
    "print(f\"\\nFound {len(citations)} citations:\")\n",
    "for citation in citations[:5]:  # Show first 5\n",
    "    print(f\"  - {citation.title}: {citation.url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Image Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Enable image generation\n",
    "image_client = OpenAIResponsesV2Client()\n",
    "\n",
    "# # Configure image output parameters\n",
    "image_client.set_image_output_params(quality=\"high\", size=\"1024x1024\", output_format=\"png\")\n",
    "\n",
    "response = image_client.create({\n",
    "    \"model\": \"gpt-4.1\",\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Generate an image of a tree with fruit\"}],\n",
    "    \"built_in_tools\": [\"image_generation\"],\n",
    "})\n",
    "\n",
    "# Extract generated images\n",
    "images = OpenAIResponsesV2Client.get_generated_images(response)\n",
    "print(f\"Generated {len(images)} image(s)\")\n",
    "\n",
    "if images:\n",
    "    # Get the data URI\n",
    "    data_uri = images[0].data_uri\n",
    "\n",
    "    # Extract base64 data (remove the \"data:image/png;base64,\" prefix)\n",
    "    if data_uri and data_uri.startswith(\"data:\"):\n",
    "        # Split on comma to get base64 data\n",
    "        base64_data = data_uri.split(\",\", 1)[1]\n",
    "\n",
    "        # Display the image\n",
    "        display(Image(data=base64.b64decode(base64_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check image generation costs\n",
    "print(f\"Image costs: ${image_client.get_image_costs():.4f}\")\n",
    "print(f\"Total costs: ${image_client.get_total_costs():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Structured Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "from autogen.llm_clients.openai_responses_v2 import OpenAIResponsesV2Client\n",
    "\n",
    "\n",
    "# Define a Pydantic model for structured output\n",
    "class Person(BaseModel):\n",
    "    name: str\n",
    "    age: int\n",
    "    occupation: str\n",
    "\n",
    "\n",
    "# Request structured output\n",
    "struct_client = OpenAIResponsesV2Client()\n",
    "\n",
    "response = struct_client.create({\n",
    "    \"model\": \"gpt-4.1\",\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Generate a fictional person's profile\"}],\n",
    "    \"response_format\": Person,\n",
    "})\n",
    "\n",
    "# Get the parsed object\n",
    "parsed = OpenAIResponsesV2Client.get_parsed_object(response)\n",
    "\n",
    "if parsed:\n",
    "    print(parsed)\n",
    "    print(\"---------------------------------\")\n",
    "    print(f\"Name: {parsed.name}\")\n",
    "    print(f\"Age: {parsed.age}\")\n",
    "    print(f\"Occupation: {parsed.occupation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 5. Cost Tracking\n",
    "\n",
    "The V2 client tracks both token costs and image generation costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_client = OpenAIResponsesV2Client()\n",
    "\n",
    "# Make several requests\n",
    "for i in range(3):\n",
    "    response = cost_client.create({\"model\": \"gpt-4.1\", \"messages\": [{\"role\": \"user\", \"content\": f\"Count to {i + 1}\"}]})\n",
    "\n",
    "    # Per-request cost\n",
    "    usage = OpenAIResponsesV2Client.get_usage(response)\n",
    "    print(f\"Request {i + 1}: {usage['total_tokens']} tokens, ${usage['cost']:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cumulative usage\n",
    "cumulative = cost_client.get_cumulative_usage()\n",
    "print(\"\\nCumulative Usage:\")\n",
    "print(f\"  Total prompt tokens: {cumulative['prompt_tokens']}\")\n",
    "print(f\"  Total completion tokens: {cumulative['completion_tokens']}\")\n",
    "print(f\"  Total tokens: {cumulative['total_tokens']}\")\n",
    "print(f\"  Token cost: ${cumulative['token_cost']:.6f}\")\n",
    "print(f\"  Image cost: ${cumulative['image_cost']:.6f}\")\n",
    "print(f\"  Total cost: ${cumulative['total_cost']:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset cost tracking\n",
    "cost_client.reset_all_costs()\n",
    "print(f\"After reset: ${cost_client.get_total_costs():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 6. V1 Backward Compatibility\n",
    "\n",
    "For code that expects ChatCompletion format, use `create_v1_compatible()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v2_client = OpenAIResponsesV2Client()\n",
    "\n",
    "# Get ChatCompletion-like response\n",
    "response = v2_client.create_v1_compatible({\"model\": \"gpt-4.1\", \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]})\n",
    "\n",
    "# Access like standard ChatCompletion\n",
    "print(f\"Type: {type(response).__name__}\")\n",
    "print(f\"Content: {response.choices[0].message.content}\")\n",
    "print(f\"Tokens: {response.usage.total_tokens}\")\n",
    "print(f\"Cost: ${response.cost:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 7. Agent Integration\n",
    "\n",
    "The V2 client integrates with AG2 agents for conversational AI workflows.\n",
    "\n",
    "## 7.1 Single Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen import ConversableAgent\n",
    "\n",
    "# Configure LLM with Responses API\n",
    "config_list = [\n",
    "    {\n",
    "        \"model\": \"gpt-5-nano\",\n",
    "        \"api_type\": \"responses_v2\",  # Use Responses API\n",
    "    }\n",
    "]\n",
    "\n",
    "llm_config = {\"config_list\": config_list}\n",
    "\n",
    "\n",
    "def math_tool(expression: str) -> str:\n",
    "    \"\"\"Evaluate a mathematical expression.\"\"\"\n",
    "    try:\n",
    "        result = eval(expression)\n",
    "        return str(result)\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "\n",
    "# Create a single assistant agent\n",
    "assistant = ConversableAgent(\n",
    "    name=\"assistant\",\n",
    "    llm_config=llm_config,\n",
    "    system_message=\"You are a helpful AI assistant who can do math. use the math_tool to do math.\",\n",
    "    functions=[math_tool],\n",
    ")\n",
    "\n",
    "assistant.register_for_execution()(math_tool)\n",
    "\n",
    "# Start a conversation\n",
    "result = assistant.run(\n",
    "    assistant,\n",
    "    message=\"use a tool to perform 2+2\",\n",
    "    max_turns=2,\n",
    ")\n",
    "result.process()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Two-Agent Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two specialized agents\n",
    "researcher = ConversableAgent(\n",
    "    name=\"researcher\",\n",
    "    llm_config=llm_config,\n",
    "    system_message=\"\"\"You are a research assistant. Your job is to:\n",
    "    1. Analyze questions thoroughly\n",
    "    2. Provide detailed, factual information\n",
    "    3. Cite sources when possible\"\"\",\n",
    ")\n",
    "\n",
    "critic = ConversableAgent(\n",
    "    name=\"critic\",\n",
    "    llm_config=llm_config,\n",
    "    system_message=\"\"\"You are a critical reviewer. Your job is to:\n",
    "    1. Review the researcher's findings\n",
    "    2. Point out any gaps or inaccuracies\n",
    "    3. Suggest improvements\n",
    "    Say 'TERMINATE' when the research is satisfactory.\"\"\",\n",
    ")\n",
    "\n",
    "# Two-agent collaboration\n",
    "response = researcher.run(\n",
    "    critic, message=\"Research the benefits and drawbacks of renewable energy sources.\", max_turns=2\n",
    ")\n",
    "response.process()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Group Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multiple specialized agents for group chat\n",
    "planner = ConversableAgent(\n",
    "    name=\"planner\",\n",
    "    llm_config=llm_config,\n",
    "    system_message=\"\"\"You are a project planner. Break down tasks into actionable steps.\n",
    "    Focus on creating clear, organized plans. Do not do any coding.\"\"\",\n",
    "    is_termination_msg=lambda x: \"TERMINATE\" in x.get(\"content\", \"\"),\n",
    ")\n",
    "\n",
    "developer = ConversableAgent(\n",
    "    name=\"developer\",\n",
    "    llm_config=llm_config,\n",
    "    system_message=\"\"\"You are a software developer. Implement solutions based on the plan.\n",
    "    Write clean, well-documented code. Do not create plans.\"\"\",\n",
    "    is_termination_msg=lambda x: \"TERMINATE\" in x.get(\"content\", \"\"),\n",
    ")\n",
    "\n",
    "reviewer = ConversableAgent(\n",
    "    name=\"reviewer\",\n",
    "    llm_config=llm_config,\n",
    "    system_message=\"\"\"You are a code reviewer. Review implementations for:\n",
    "    1. Correctness\n",
    "    2. Best practices\n",
    "    3. Potential improvements\n",
    "    Say 'TERMINATE' when the solution is complete and reviewed.\"\"\",\n",
    "    is_termination_msg=lambda x: \"TERMINATE\" in x.get(\"content\", \"\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create group chat\n",
    "from autogen.agentchat import run_group_chat\n",
    "from autogen.agentchat.group.patterns import AutoPattern\n",
    "\n",
    "pattern = AutoPattern(\n",
    "    initial_agent=planner,\n",
    "    agents=[planner, developer, reviewer],\n",
    "    group_manager_args={\"llm_config\": llm_config},\n",
    ")\n",
    "\n",
    "response = run_group_chat(\n",
    "    pattern=pattern,\n",
    "    messages=\"Create a Python function that calculates the Fibonacci sequence up to n terms.\",\n",
    "    max_rounds=4,\n",
    ")\n",
    "\n",
    "response.process()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 8. Advanced: Custom Function Tools\n",
    "\n",
    "Combine built-in tools with custom function tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom tools\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"Get the current weather for a city.\"\"\"\n",
    "    # Mock implementation\n",
    "    return f\"The weather in {city} is sunny, 72Â°F\"\n",
    "\n",
    "\n",
    "def calculate(expression: str) -> str:\n",
    "    \"\"\"Evaluate a mathematical expression.\"\"\"\n",
    "    try:\n",
    "        result = eval(expression)\n",
    "        return str(result)\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "\n",
    "# Define tool schemas\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_weather\",\n",
    "            \"description\": \"Get current weather for a city\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\"city\": {\"type\": \"string\", \"description\": \"City name\"}},\n",
    "                \"required\": [\"city\"],\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"calculate\",\n",
    "            \"description\": \"Evaluate a math expression\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\"expression\": {\"type\": \"string\", \"description\": \"Math expression\"}},\n",
    "                \"required\": [\"expression\"],\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "]\n",
    "\n",
    "print(\"Tools defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen.llm_clients.openai_responses_v2 import OpenAIResponsesV2Client, TextContent, ToolCallContent\n",
    "\n",
    "# Use custom tools with the V2 client\n",
    "tools_client = OpenAIResponsesV2Client()\n",
    "\n",
    "response = tools_client.create({\n",
    "    \"model\": \"gpt-4o-mini\",\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"What's 25 * 4 + 10?\"}],\n",
    "    \"tools\": tools,\n",
    "})\n",
    "\n",
    "# Check for tool calls\n",
    "for msg in response.messages:\n",
    "    for block in msg.content:\n",
    "        if isinstance(block, ToolCallContent):\n",
    "            print(f\"Tool call: {block.name}({block.arguments})\")\n",
    "        elif isinstance(block, TextContent):\n",
    "            print(f\"Text: {block.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Summary\n",
    "\n",
    "The `OpenAIResponsesV2Client` provides:\n",
    "\n",
    "| Feature | Description |\n",
    "|---------|-------------|\n",
    "| **Stateful Conversations** | Automatic context tracking via `previous_response_id` |\n",
    "| **Rich Content Blocks** | TextContent, ReasoningContent, CitationContent, ImageContent, ToolCallContent |\n",
    "| **Built-in Tools** | Web search, image generation, apply_patch |\n",
    "| **Multimodal Support** | Send and receive images |\n",
    "| **Structured Output** | Pydantic models and JSON schemas |\n",
    "| **Cost Tracking** | Token and image generation cost tracking |\n",
    "| **V1 Compatibility** | `create_v1_compatible()` for ChatCompletion format |\n",
    "| **Agent Integration** | Works with AG2 single, two-agent, and group chat |\n",
    "\n",
    "For more information, see the [AG2 documentation](https://docs.ag2.ai)."
   ]
  }
 ],
 "metadata": {
  "front_matter": {
   "description": "OpenAI Responses API V2 Client with rich UnifiedResponse objects, stateful conversations, and agent integration",
   "tags": [
    "openai",
    "responses-api",
    "v2-client",
    "multimodal",
    "agents"
   ]
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
