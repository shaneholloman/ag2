{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VL3_XWS1CBPW"
   },
   "source": [
    "# RemyxCodeExecutor: Agentic Code Exploration and Execution\n",
    "\n",
    "*Check out the [Remyx Docs](https://docs.remyx.ai) for new utilities released!*\n",
    "\n",
    "**Discover â†’ Explore â†’ Experiment** with novel codebases through AI-guided exploration of Remyx-built Docker images.\n",
    "\n",
    "## The Problem\n",
    "\n",
    "Experimenting with novel codebases means hours resolving dependencies (CUDA versions, conflicting packages, undocumented requirements) plus manual code archaeology to find what matters in unfamiliar repos.\n",
    "\n",
    "## The Solution\n",
    "\n",
    "RemyxCodeExecutor provides:\n",
    "\n",
    "**Remyx-built Docker images** that reproduce codebases from 7000+ arXiv papers (expanding to other resources):\n",
    "* Complete environments with dependencies resolved\n",
    "* Pull and execute immediately\n",
    "* Reproduce and extend codebases to your use case\n",
    "\n",
    "**AI agents via AG2 (AutoGen)** that guide exploration:\n",
    "* Navigate repos and identify key implementations\n",
    "* Explain architecture and core logic\n",
    "* Execute code and interpret results\n",
    "* Modify for custom experiments\n",
    "\n",
    "This notebook shows you how to search for Docker images, launch AI-guided exploration, and run experiments on reproduced codebases in minutes instead of hours.\n",
    "\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Install AG2 with Remyx support:\n",
    "```bash\n",
    "pip install ag2[remyx]\n",
    "```\n",
    "\n",
    "Make sure you have the following dependencies:\n",
    "* [Remyx AI API key](https://docs.remyx.ai/cli)\n",
    "* [Docker](https://www.docker.com/get-started/)\n",
    "* [OpenAI API key for LLM agents](https://platform.openai.com/api-keys)\n",
    "\n",
    "Set your API tokens as environment variables:\n",
    "\n",
    "```bash\n",
    "export REMYXAI_API_KEY=your_remyxai_token\n",
    "export OPENAI_API_KEY=your_openai_key\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fwSPHojGGlU6"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Ensure you have your API tokens set\n",
    "assert os.getenv(\"REMYXAI_API_KEY\"), \"Please set REMYXAI_API_KEY environment variable\"\n",
    "assert os.getenv(\"OPENAI_API_KEY\"), \"Please set OPENAI_API_KEY environment variable\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RCGmsoFxG0Y_"
   },
   "source": [
    "## Step 1: Discover Papers\n",
    "\n",
    "Search 1000+ research papers with pre-built Docker environments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VtruuzoIG7Jx"
   },
   "outputs": [],
   "source": [
    "from remyxai.client.search import SearchClient\n",
    "\n",
    "client = SearchClient()\n",
    "query = \"CLIP semantic alignment\"\n",
    "\n",
    "# Search for papers\n",
    "papers = client.search(query=query, has_docker=True, max_results=5)\n",
    "\n",
    "# Browse results\n",
    "for paper in papers:\n",
    "    print(f\"ðŸ“– {paper.title[:50]}...\")\n",
    "    print(f\"   arXiv: {paper.arxiv_id}\")\n",
    "    print(f\"   image: {paper.docker_image}\")\n",
    "    print(f\"   abstract: {paper.abstract}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JD--eorYHTo0"
   },
   "source": [
    "You'll see results like:\n",
    "```python\n",
    "ðŸ“– CLIPin: A Non-contrastive Plug-in to CLIP for Mult...\n",
    "   arXiv: 2508.06434v1\n",
    "   image: remyxai/2508.06434v1:latest\n",
    "   abstract: Large-scale natural image-text datasets, especially those automatically\n",
    "collected from the web, often suffer from loose semantic alignment due to weak\n",
    "supervision, while medical datasets tend to have high cross-modal correlation\n",
    "but low content diversity. These properties pose a common challenge for\n",
    "contrastive language-image pretraining (CLIP): they hinder the model's ability\n",
    "to learn robust and generalizable representations. In this work, we propose\n",
    "CLIPin, a unified non-contrastive plug-in th\n",
    "\n",
    "ðŸ“– COOkeD: Ensemble-based OOD detection in the era of...\n",
    "   arXiv: 2507.22576v1\n",
    "   image: remyxai/2507.22576v1:latest\n",
    "   abstract: Out-of-distribution (OOD) detection is an important building block in\n",
    "trustworthy image recognition systems as unknown classes may arise at\n",
    "test-time. OOD detection methods typically revolve around a single classifier,\n",
    "leading to a split in the research field between the classical supervised\n",
    "setting (e.g. ResNet18 classifier trained on CIFAR100) vs. the zero-shot\n",
    "setting (class names fed as prompts to CLIP). In both cases, an overarching\n",
    "challenge is that the OOD detection performance is implici\n",
    "\n",
    "ðŸ“– Mammo-CLIP Dissect: A Framework for Analysing Mamm...\n",
    "   arXiv: 2509.21102v1\n",
    "   image: remyxai/2509.21102v1:latest\n",
    "   abstract: Understanding what deep learning (DL) models learn is essential for the safe\n",
    "deployment of artificial intelligence (AI) in clinical settings. While previous\n",
    "work has focused on pixel-based explainability methods, less attention has been\n",
    "paid to the textual concepts learned by these models, which may better reflect\n",
    "the reasoning used by clinicians. We introduce Mammo-CLIP Dissect, the first\n",
    "concept-based explainability framework for systematically dissecting DL vision\n",
    "models trained for mammograp\n",
    "\n",
    "ðŸ“– CLASP: General-Purpose Clothes Manipulation with S...\n",
    "   arXiv: 2507.19983v1\n",
    "   image: remyxai/2507.19983v1:latest\n",
    "   abstract: Clothes manipulation, such as folding or hanging, is a critical capability\n",
    "for home service robots. Despite recent advances, most existing methods remain\n",
    "limited to specific tasks and clothes types, due to the complex,\n",
    "high-dimensional geometry of clothes. This paper presents CLothes mAnipulation\n",
    "with Semantic keyPoints (CLASP), which aims at general-purpose clothes\n",
    "manipulation over different clothes types, T-shirts, shorts, skirts, long\n",
    "dresses, ... , as well as different tasks, folding, flatt\n",
    "\n",
    "ðŸ“– Personalized Education with Ranking Alignment Reco...\n",
    "   arXiv: 2507.23664v1\n",
    "   image: remyxai/2507.23664v1:latest\n",
    "   abstract: Personalized question recommendation aims to guide individual students\n",
    "through questions to enhance their mastery of learning targets. Most previous\n",
    "methods model this task as a Markov Decision Process and use reinforcement\n",
    "learning to solve, but they struggle with efficient exploration, failing to\n",
    "identify the best questions for each student during training. To address this,\n",
    "we propose Ranking Alignment Recommendation (RAR), which incorporates\n",
    "collaborative ideas into the exploration mechanism,\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zz45Kyy7HbiU"
   },
   "source": [
    "## Step 2: Fast Exploration\n",
    "\n",
    "You can quickly explore the contents of the codebase and environment using the `explore()` method of the `RemyxCodeExecutor`.\n",
    "\n",
    "How it works:\n",
    "\n",
    "1. Pulls Docker image with paper's code and dependencies\n",
    "2. Creates AI agents (one explores, one executes)\n",
    "3. Interactive session starts - you guide the exploration\n",
    "4. Ask free form questions about the code, create your own tests, and expand upon the research!\n",
    "\n",
    "You can launch an interactive session where you are able to chat with the system of agents or run automatically without pausing to run default tests and exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Zr0KN4BIh1L"
   },
   "source": [
    "### Quick Start (Default Exploration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0l-flTBWIerW"
   },
   "outputs": [],
   "source": [
    "from autogen.coding import RemyxCodeExecutor\n",
    "\n",
    "arxiv_id = papers[0].arxiv_id\n",
    "executor = RemyxCodeExecutor(arxiv_id=arxiv_id)\n",
    "executor.explore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vw6bihSbIk9N"
   },
   "source": [
    "### Batch Mode (Automated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NVjIa_86IpwA"
   },
   "outputs": [],
   "source": [
    "# Runs automatically without pausing\n",
    "executor = RemyxCodeExecutor(arxiv_id=arxiv_id)\n",
    "result = executor.explore(goal=\"Run the default example quickstart\", interactive=False, max_turns=10)\n",
    "\n",
    "print(f\"âœ… Completed {len(result.chat_history)} steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "31fNgB29cLfA"
   },
   "source": [
    "## Real-world Example: Exploring CLIPin\n",
    "Let's explore the [CLIPin paper](https://arxiv.org/pdf/2508.06434) - a method that improves CLIP's semantic alignment using non-contrastive learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AQXp33LOGvXq"
   },
   "outputs": [],
   "source": [
    "from autogen.coding import RemyxCodeExecutor\n",
    "\n",
    "# Create executor for CLIPin paper\n",
    "executor = RemyxCodeExecutor(arxiv_id=arxiv_id)\n",
    "\n",
    "# Start interactive exploration\n",
    "result = executor.explore(\n",
    "    goal=\"\"\"Explore CLIPin step-by-step:\n",
    "\n",
    "    Phase 1: Understanding\n",
    "    - Show repository structure\n",
    "    - Read the README\n",
    "    - Find the CLIPin model code\n",
    "\n",
    "    Phase 2: Architecture\n",
    "    - Explain the non-contrastive approach\n",
    "    - Show the loss function\n",
    "    - Compare with standard CLIP\n",
    "\n",
    "    Work step-by-step. Explain clearly.\n",
    "    \"\"\",\n",
    "    interactive=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DAv0b9NwcoQW"
   },
   "source": [
    "You can expect the output after multiple turns to look like:\n",
    "```python\n",
    "...\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "Replying as research_explorer. Provide feedback to code_executor. Press enter to skip and use auto-reply, or type 'exit' to end the conversation:\n",
    "\n",
    ">>>>>>>> NO HUMAN INPUT RECEIVED.\n",
    "\n",
    ">>>>>>>> USING AUTO REPLY...\n",
    "research_explorer (to code_executor):\n",
    "\n",
    "The `model.py` file contains the architecture of the CLIPin model, which expands upon the original CLIP architecture. Hereâ€™s a summary of the key components and classes that it implements:\n",
    "\n",
    "### Key Components and Classes:\n",
    "\n",
    "1. **Bottleneck Class**:\n",
    "   - Implements a residual block used in building the ResNet architecture. Each block consists of three convolutional layers and has a skip connection to facilitate training deeper networks.\n",
    "\n",
    "2. **AttentionPool2d Class**:\n",
    "   - Implements an attention pooling layer that uses multi-head attention to aggregate features spatially, enhancing the model's capability to capture relationships in the data.\n",
    "\n",
    "3. **ModifiedResNet Class**:\n",
    "   - Implements a modified version of the ResNet architecture tailored for the model. It features three \"stem\" convolutions and an attention-based pooling layer instead of an average pool at the end.\n",
    "\n",
    "4. **Transformers and Vision Transformers**:\n",
    "   - **Transformer Class**: Implements multi-layer transformer blocks equipped with residual connections.\n",
    "   - **VisionTransformer Class**: Specializes in extracting features from images and integrates a transformer mechanism for better contextual understanding.\n",
    "\n",
    "5. **TextEncoder Class**:\n",
    "   - Encodes textual data using embeddings and a transformer architecture. It incorporates positional embeddings to maintain the order of the tokens.\n",
    "\n",
    "6. **CLIP Class**:\n",
    "   - This is the main class that integrates both vision and text encoders, using components previously defined.\n",
    "   - It implements various projections and transformation layers that were specifically tailored for contrastive and non-contrastive learning approaches.\n",
    "   - This class also contains the forward method which computes embeddings for images and text, as well as similarity metrics.\n",
    "\n",
    "7. **Initialization and Parameter Management**:\n",
    "   - Several methods handle initialization, copying of parameters for momentum models, and setting grad checkpointing to reduce memory usage.\n",
    "   - The `initialize_parameters` method sets the correct weights for different sections in the model.\n",
    "\n",
    "8. **Utility Functions**:\n",
    "   - Functions like `convert_weights` convert the model parameters to half precision for performance optimization during inference.\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "Now that we have outlined the structure and functionality of the CLIPin model, the next phase is to understand its non-contrastive approach and how it compares with the standard CLIP model in terms of loss function and training strategy.\n",
    "\n",
    "Shall we explore its non-contrastive approach next?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tfbd73mddG75"
   },
   "source": [
    "## Building on Research\n",
    "Use paper code as starting point for your own projects and research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B8J1HoYtdRB3"
   },
   "outputs": [],
   "source": [
    "from autogen import ConversableAgent\n",
    "from autogen.coding import RemyxCodeExecutor\n",
    "\n",
    "# Start with paper's environment\n",
    "executor = RemyxCodeExecutor(arxiv_id=arxiv_id)\n",
    "\n",
    "# Create your own agent for custom experiments\n",
    "agent = ConversableAgent(\n",
    "    \"my_researcher\", llm_config=False, code_execution_config={\"executor\": executor}, human_input_mode=\"NEVER\"\n",
    ")\n",
    "\n",
    "# Run your custom code in paper's environment\n",
    "agent.generate_reply(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"\"\"```python\n",
    "# Your custom experiment here\n",
    "from clip.model import CLIPin\n",
    "model = CLIPin.load_pretrained()\n",
    "# ... your modifications ...\n",
    "```\"\"\",\n",
    "        }\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VS49XQuPddX2"
   },
   "source": [
    "## Advanced Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P_1GWm7sdgeh"
   },
   "source": [
    "### Custom Docker Args\n",
    "You can pass additional args to `container_create_kwargs` for further customization and configuration of containers like passing additional environment variables or switching to a GPU enabled container runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lcRXrX2idvrr"
   },
   "outputs": [],
   "source": [
    "executor = RemyxCodeExecutor(\n",
    "    arxiv_id=arxiv_id,\n",
    "    timeout=600,\n",
    "    container_create_kwargs={\n",
    "        \"environment\": {\n",
    "            \"HF_TOKEN\": os.getenv(\"HF_TOKEN\"),\n",
    "            \"WANDB_API_KEY\": os.getenv(\"WANDB_API_KEY\"),\n",
    "        },\n",
    "        \"mem_limit\": \"16g\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZgyrEnvsdyS6"
   },
   "source": [
    "### Paper Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M5cx_BSMd1Z0"
   },
   "outputs": [],
   "source": [
    "executor = RemyxCodeExecutor(arxiv_id=arxiv_id)\n",
    "context = executor.get_paper_context()\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "84yEda5zd258"
   },
   "source": [
    "### Direct Use of Docker Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "huxugCkMd7C0"
   },
   "outputs": [],
   "source": [
    "# If you know the image name\n",
    "executor = RemyxCodeExecutor(image=\"remyxai/2508.06434v1:latest\", timeout=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uCmYpFlGd_2Y"
   },
   "source": [
    "### Manual Agent Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vJNip66YeC9G"
   },
   "outputs": [],
   "source": [
    "# For advanced users who want full control\n",
    "executor = RemyxCodeExecutor(arxiv_id=arxiv_id)\n",
    "executor_agent, writer_agent = executor.create_agents(goal=\"Custom exploration\", llm_model=\"gpt-4o\")\n",
    "\n",
    "# Customize the chat\n",
    "result = executor_agent.initiate_chat(writer_agent, message=\"Begin exploring the contents in /app\", max_turns=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KyCvKrGEevXI"
   },
   "source": [
    "## Tips & Tricks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D4tjWWa-ezk5"
   },
   "source": [
    "**Start with Search**\n",
    "\n",
    "You can quickly browse the catalog of pre-built images for papers you may want to experiment. Search papers and prebuilt Docker images using full text, keywords, or arXiv IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EkMp01FBhbV-"
   },
   "outputs": [],
   "source": [
    "from remyxai.client.search import SearchClient\n",
    "\n",
    "papers = SearchClient().search(query=\"data synthesis techniques\", has_docker=True, max_results=10)\n",
    "\n",
    "for p in papers:\n",
    "    print(f\"{p.arxiv_id}: {p.title[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lu1irZGfhpEz"
   },
   "source": [
    "**Use Interactive Mode for Learning**\n",
    "\n",
    "Pause at each step to guide the agents in your exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9VL70RYWhwqQ"
   },
   "outputs": [],
   "source": [
    "executor.explore(\n",
    "    goal=\"Explain this paper's approach\",\n",
    "    interactive=True,  # Lets you guide each step\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nlPhOC0Bh076"
   },
   "source": [
    "**Use Batch Mode for Experiments**\n",
    "\n",
    "Expand your experimentation by running multiple papers automatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IV74ihneh_1D"
   },
   "outputs": [],
   "source": [
    "paper_ids = [\"2508.06434v1\", \"2103.00020v1\", \"2010.11929v2\"]\n",
    "\n",
    "results = {}\n",
    "for arxiv_id in paper_ids:\n",
    "    executor = RemyxCodeExecutor(arxiv_id=arxiv_id)\n",
    "    result = executor.explore(goal=\"Run quickstart\", interactive=False, verbose=False)\n",
    "    results[arxiv_id] = result\n",
    "\n",
    "# Compare results\n",
    "for arxiv_id, result in results.items():\n",
    "    print(f\"{arxiv_id}: {len(result.chat_history)} steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nzpRpsotiOgf"
   },
   "source": [
    "**Check Metadata**\n",
    "\n",
    "Get a quick summary of all the available resources for a paper you may be interested in exploring further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z7PBB-02iOzr"
   },
   "outputs": [],
   "source": [
    "executor = RemyxCodeExecutor(arxiv_id=arxiv_id)\n",
    "print(executor.get_paper_context())\n",
    "# Shows: title, GitHub, working directory, quickstart hints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gv79n3O5jwei"
   },
   "source": [
    "## Summary\n",
    "\n",
    "This notebook showed you how RemyxCodeExecutor transforms research paper execution:\n",
    "\n",
    "**Three Powerful Modes:**\n",
    "-  **Quick Start**: `executor.explore()` - AI-guided exploration with defaults\n",
    "-  **Learning Mode**: Interactive step-by-step with custom goals\n",
    "-  **Batch Mode**: Automated experiments across multiple papers\n",
    "\n",
    "**What Makes It Special:**\n",
    "- Pre-configured Docker environments for 1000+ papers\n",
    "- Zero dependency setup (everything pre-installed)\n",
    "- AI agents that explain as they explore\n",
    "- Reproducible execution every time\n",
    "\n",
    "\n",
    "### Quick Reference\n",
    "```python\n",
    "# 1. Search\n",
    "from remyxai.client.search import SearchClient\n",
    "papers = SearchClient().search(\"your topic\", has_docker=True)\n",
    "\n",
    "# 2. Create executor\n",
    "from autogen.coding import RemyxCodeExecutor\n",
    "executor = RemyxCodeExecutor(arxiv_id=papers[0].arxiv_id)\n",
    "\n",
    "# 3. Explore (pick one mode)\n",
    "executor.explore()                                   # Quick start\n",
    "executor.explore(goal=\"...\", interactive=True)       # Learning\n",
    "executor.explore(goal=\"...\", interactive=False)      # Batch"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
