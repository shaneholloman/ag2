{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# OpenTelemetry Tracing for AG2 Agents\n",
    "\n",
    "AG2 provides built-in [OpenTelemetry](https://opentelemetry.io/) instrumentation for tracing multi-agent conversations. This lets you observe agent interactions, LLM calls, tool executions, and more using any OpenTelemetry-compatible backend (Jaeger, Grafana Tempo, Datadog, etc.).\n",
    "\n",
    "````{=mdx}\n",
    ":::info Requirements\n",
    "Install AG2 with tracing support:\n",
    "\n",
    "```bash\n",
    "pip install \"ag2[openai,tracing]\"\n",
    "```\n",
    "\n",
    "For this notebook we use `ConsoleSpanExporter` so no external backend is needed.\n",
    ":::\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Setting Up OpenTelemetry\n",
    "\n",
    "First, configure a `TracerProvider` with a `ConsoleSpanExporter`. This prints spans directly to stdout, which is perfect for learning. In production, you would replace this with an OTLP exporter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from opentelemetry import trace\n",
    "from opentelemetry.sdk.resources import Resource\n",
    "from opentelemetry.sdk.trace import TracerProvider\n",
    "from opentelemetry.sdk.trace.export import ConsoleSpanExporter, SimpleSpanProcessor\n",
    "\n",
    "resource = Resource.create(attributes={\"service.name\": \"ag2-tracing-notebook\"})\n",
    "tracer_provider = TracerProvider(resource=resource)\n",
    "tracer_provider.add_span_processor(SimpleSpanProcessor(ConsoleSpanExporter()))\n",
    "trace.set_tracer_provider(tracer_provider)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Creating and Instrumenting Agents\n",
    "\n",
    "Create agents as usual, then call `instrument_llm_wrapper` (once, globally) and `instrument_agent` (per agent) to enable tracing. No changes to your agent code are needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen import ConversableAgent, LLMConfig\n",
    "from autogen.opentelemetry import instrument_agent, instrument_llm_wrapper\n",
    "\n",
    "llm_config = LLMConfig(api_type=\"openai\", model=\"gpt-4o-mini\")\n",
    "\n",
    "assistant = ConversableAgent(\n",
    "    \"assistant\",\n",
    "    system_message=\"You are a helpful assistant. Reply concisely in one or two sentences.\",\n",
    "    llm_config=llm_config,\n",
    "    human_input_mode=\"NEVER\",\n",
    ")\n",
    "\n",
    "user_proxy = ConversableAgent(\n",
    "    \"user_proxy\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    llm_config=False,\n",
    "    max_consecutive_auto_reply=0,\n",
    ")\n",
    "\n",
    "# Instrument the LLM wrapper (global, once) and each agent\n",
    "instrument_llm_wrapper(tracer_provider=tracer_provider)\n",
    "instrument_agent(assistant, tracer_provider=tracer_provider)\n",
    "instrument_agent(user_proxy, tracer_provider=tracer_provider)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Two-Agent Chat with Tracing\n",
    "\n",
    "Run a simple chat. The console output will show the spans emitted during the conversation. The trace hierarchy looks like:\n",
    "\n",
    "```\n",
    "conversation \"user_proxy\" -> \"assistant\"\n",
    "  └── invoke_agent \"assistant\"\n",
    "        └── chat gpt-4o-mini\n",
    "```\n",
    "\n",
    "Key attributes on each span include `ag2.span.type`, `gen_ai.operation.name`, `gen_ai.agent.name`, and token usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = user_proxy.initiate_chat(\n",
    "    assistant,\n",
    "    message=\"What are the three primary colors?\",\n",
    "    max_turns=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Tracing Tool Execution\n",
    "\n",
    "When agents use tools, tool execution spans are automatically captured as children of the agent span. The trace hierarchy expands to:\n",
    "\n",
    "```\n",
    "conversation \"tool_user\" -> \"tool_agent\"\n",
    "  ├── invoke_agent \"tool_agent\"\n",
    "  │     └── chat gpt-4o-mini          (LLM decides to call tool)\n",
    "  ├── invoke_agent \"tool_user\"\n",
    "  │     └── execute_tool get_weather   (tool execution)\n",
    "  └── invoke_agent \"tool_agent\"\n",
    "        └── chat gpt-4o-mini          (LLM processes tool result)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "\n",
    "def get_weather(city: Annotated[str, \"The city to get weather for\"]) -> str:\n",
    "    \"\"\"Get the current weather for a city.\"\"\"\n",
    "    return f\"The weather in {city} is sunny, 72F.\"\n",
    "\n",
    "\n",
    "tool_agent = ConversableAgent(\n",
    "    \"tool_agent\",\n",
    "    system_message=\"Use the get_weather tool to answer weather questions. Reply concisely.\",\n",
    "    llm_config=llm_config,\n",
    "    human_input_mode=\"NEVER\",\n",
    ")\n",
    "\n",
    "tool_user = ConversableAgent(\n",
    "    \"tool_user\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    llm_config=False,\n",
    "    max_consecutive_auto_reply=2,\n",
    ")\n",
    "\n",
    "tool_agent.register_for_llm()(get_weather)\n",
    "tool_user.register_for_execution()(get_weather)\n",
    "\n",
    "instrument_agent(tool_agent, tracer_provider=tracer_provider)\n",
    "instrument_agent(tool_user, tracer_provider=tracer_provider)\n",
    "\n",
    "result = tool_user.initiate_chat(\n",
    "    tool_agent,\n",
    "    message=\"What's the weather in Tokyo?\",\n",
    "    max_turns=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Tracing Group Chats\n",
    "\n",
    "For group chat patterns, use `instrument_pattern` to automatically instrument all agents in the group, including the GroupChatManager and speaker selection logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen.agentchat import initiate_group_chat\n",
    "from autogen.agentchat.group.patterns.auto import AutoPattern\n",
    "from autogen.opentelemetry import instrument_pattern\n",
    "\n",
    "planner = ConversableAgent(\n",
    "    \"planner\",\n",
    "    system_message=\"You plan tasks. Be concise and reply in one sentence.\",\n",
    "    llm_config=llm_config,\n",
    "    human_input_mode=\"NEVER\",\n",
    ")\n",
    "executor = ConversableAgent(\n",
    "    \"executor\",\n",
    "    system_message=\"You execute planned tasks. Be concise and reply in one sentence.\",\n",
    "    llm_config=llm_config,\n",
    "    human_input_mode=\"NEVER\",\n",
    ")\n",
    "\n",
    "group_user = ConversableAgent(\n",
    "    \"group_user\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    llm_config=False,\n",
    ")\n",
    "\n",
    "pattern = AutoPattern(\n",
    "    initial_agent=planner,\n",
    "    agents=[planner, executor],\n",
    "    user_agent=group_user,\n",
    "    group_manager_args={\"llm_config\": llm_config},\n",
    ")\n",
    "\n",
    "instrument_pattern(pattern, tracer_provider=tracer_provider)\n",
    "\n",
    "result, context, last_agent = initiate_group_chat(\n",
    "    pattern=pattern,\n",
    "    messages=\"Write a haiku about coding.\",\n",
    "    max_rounds=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Sending Traces to a Backend\n",
    "\n",
    "For production use, replace `ConsoleSpanExporter` with an OTLP exporter to send traces to Jaeger, Grafana Tempo, Datadog, or any OpenTelemetry-compatible backend.\n",
    "\n",
    "```python\n",
    "from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter\n",
    "from opentelemetry.sdk.trace.export import BatchSpanProcessor\n",
    "\n",
    "exporter = OTLPSpanExporter(endpoint=\"http://localhost:4317\")\n",
    "tracer_provider = TracerProvider(resource=resource)\n",
    "tracer_provider.add_span_processor(BatchSpanProcessor(exporter))\n",
    "trace.set_tracer_provider(tracer_provider)\n",
    "```\n",
    "\n",
    "A docker-compose stack with Grafana Tempo and OpenTelemetry Collector is included in the `tracing/` directory of the AG2 repo:\n",
    "\n",
    "```bash\n",
    "cd tracing && docker-compose up -d\n",
    "# Grafana UI: http://localhost:3333\n",
    "# OTLP gRPC endpoint: localhost:14317\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "AG2's OpenTelemetry integration gives you full observability into multi-agent workflows:\n",
    "\n",
    "- **`instrument_agent`** traces conversations, agent replies, tool calls, code execution, and human input\n",
    "- **`instrument_llm_wrapper`** traces all LLM API calls with token usage and cost\n",
    "- **`instrument_pattern`** traces group chats including speaker selection\n",
    "- All spans follow the [OpenTelemetry GenAI Semantic Conventions](https://opentelemetry.io/docs/specs/semconv/gen-ai/gen-ai-agent-spans/)\n",
    "\n",
    "For more details, see the [Tracing documentation](https://docs.ag2.ai/user-guide/tracing)."
   ]
  }
 ],
 "metadata": {
  "front_matter": {
   "description": "Tracing AG2 agents with OpenTelemetry: instrument agents, LLM calls, and tool execution to observe multi-agent interactions.",
   "tags": [
    "observability",
    "tools",
    "tracing",
    "opentelemetry",
    "jaeger",
    "grafana tempo",
    "datadog"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
